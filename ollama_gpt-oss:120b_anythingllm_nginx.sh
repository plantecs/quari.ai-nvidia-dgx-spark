#!/usr/bin/env bash
set -Eeuo pipefail
trap 'echo "ERROR on line $LINENO: $BASH_COMMAND" >&2' ERR

echo "==================================================================="
echo " DGX Spark Setup: Ollama (GPU) + AnythingLLM + HTTPS"
echo "==================================================================="
echo "Runs Ollama as the backend, AnythingLLM as the frontend, nginx for TLS."
echo "Models cache on disk and survive restarts."
echo

# -------------------------------------------------------------------
# REQUIREMENTS
# -------------------------------------------------------------------
for cmd in docker openssl curl; do
  if ! command -v "$cmd" >/dev/null 2>&1; then
    echo "ERROR: '$cmd' is required but not installed."
    exit 1
  fi
done

if docker compose version >/dev/null 2>&1; then
  DOCKER_COMPOSE_CMD="docker compose"
else
  if ! command -v docker-compose >/dev/null 2>&1; then
    echo "ERROR: docker compose (v2) or docker-compose (v1) is required."
    exit 1
  fi
  DOCKER_COMPOSE_CMD="docker-compose"
fi

# -------------------------------------------------------------------
# DEFAULTS (override via env)
# -------------------------------------------------------------------
HOST_FQDN="$(hostname -f 2>/dev/null || hostname)"
MAX_WAIT_MINUTES="${MAX_WAIT_MINUTES:-360}"     # generous for first model pull
POLL_INTERVAL_SEC="${POLL_INTERVAL_SEC:-5}"

# Default to the Ollama gpt-oss model (120B, MXFP4 ~65GB, 128k context).
# Override via env OLLAMA_MODEL=... if you want a smaller variant.
OLLAMA_MODEL="${OLLAMA_MODEL:-gpt-oss:120b}"
OLLAMA_EMBED_MODEL="${OLLAMA_EMBED_MODEL:-nomic-embed-text:latest}"
OLLAMA_CONTEXT_TOKENS="${OLLAMA_CONTEXT_TOKENS:-128000}"
OLLAMA_MAX_TOKENS="${OLLAMA_MAX_TOKENS:-2000}"
OLLAMA_BASE_URL="${OLLAMA_BASE_URL:-http://ollama:11434}"

UID_VAL="$(id -u)"
GID_VAL="$(id -g)"

# -------------------------------------------------------------------
# HOSTNAME + SECRETS
# -------------------------------------------------------------------
echo
echo "Using hostname '${HOST_FQDN}' for TLS certificate CN and nginx server_name."
echo "Generating secrets (JWT, AnythingLLM admin password)..."

JWT_SECRET="$(openssl rand -hex 64)"
ADMIN_PASSWORD="$(openssl rand -base64 32 | tr -dc 'A-Za-z0-9' | head -c 24)"

# -------------------------------------------------------------------
# ENV FILES
# -------------------------------------------------------------------
cat > .env <<EOF
# Auto-generated by setup_ollama_anythingllm.sh
UID=${UID_VAL}
GID=${GID_VAL}
EOF

cat > anythingllm.env <<EOF
# Auto-generated by setup_ollama_anythingllm.sh

AUTH_TOKEN=${ADMIN_PASSWORD}
JWT_SECRET=${JWT_SECRET}
STORAGE_DIR=/app/server/storage

# Ollama backend
LLM_PROVIDER=ollama
OLLAMA_BASE_PATH=${OLLAMA_BASE_URL}
OLLAMA_MODEL_PREF=${OLLAMA_MODEL}
OLLAMA_MODEL_TOKEN_LIMIT=${OLLAMA_CONTEXT_TOKENS}
OLLAMA_MAX_TOKENS=${OLLAMA_MAX_TOKENS}

# Embeddings via Ollama
EMBEDDING_ENGINE=ollama
EMBEDDING_BASE_PATH=${OLLAMA_BASE_URL}
EMBEDDING_MODEL_PREF=${OLLAMA_EMBED_MODEL}
EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192

# Vector DB (AnythingLLM will init its own; change if desired)
VECTOR_DB=lancedb
EOF

echo "Created .env and anythingllm.env"

# -------------------------------------------------------------------
# SSL CERT FOR NGINX
# -------------------------------------------------------------------
mkdir -p nginx/certs
echo "Generating self-signed TLS certificate..."
openssl req -x509 -nodes -days 365 \
  -newkey rsa:4096 \
  -keyout nginx/certs/anythingllm.key \
  -out nginx/certs/anythingllm.crt \
  -subj "/CN=${HOST_FQDN}" >/dev/null 2>&1

# -------------------------------------------------------------------
# NGINX CONFIG
# -------------------------------------------------------------------
mkdir -p nginx
cat > nginx/nginx.conf <<EOF
user  nginx;
worker_processes  auto;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    sendfile      on;
    keepalive_timeout  65;

    # HTTP -> HTTPS redirect
    server {
        listen 80;
        server_name ${HOST_FQDN};
        return 301 https://\$host\$request_uri;
    }

    # HTTPS reverse proxy to AnythingLLM
    server {
        listen 443 ssl;
        server_name ${HOST_FQDN};

        ssl_certificate     /etc/nginx/certs/anythingllm.crt;
        ssl_certificate_key /etc/nginx/certs/anythingllm.key;

        ssl_protocols       TLSv1.2 TLSv1.3;
        ssl_ciphers         HIGH:!aNULL:!MD5;

        location / {
            proxy_pass http://anythingllm:3001;
            proxy_http_version 1.1;

            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
            proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;

            proxy_buffering off;
        }
    }
}
EOF

# -------------------------------------------------------------------
# DIRECTORIES
# -------------------------------------------------------------------
mkdir -p anythingllm_data
mkdir -p ollama_data        # persistent model cache

# -------------------------------------------------------------------
# DOCKER-COMPOSE (Ollama + AnythingLLM + nginx)
# -------------------------------------------------------------------
cat > docker-compose.yml <<'EOF'
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    shm_size: "16g"
    gpus: "all"
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
    volumes:
      - ./ollama_data:/root/.ollama
    ports:
      - "127.0.0.1:11434:11434"
    networks:
      - llm-net

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    restart: unless-stopped
    cap_add:
      - SYS_ADMIN
    user: "${UID}:${GID}"
    env_file:
      - anythingllm.env
    environment:
      STORAGE_DIR: /app/server/storage
    volumes:
      - ./anythingllm_data:/app/server/storage
      - ./anythingllm.env:/app/server/.env
    depends_on:
      - ollama
    networks:
      - llm-net

  nginx:
    image: nginx:alpine
    container_name: anythingllm-nginx
    restart: unless-stopped
    depends_on:
      - anythingllm
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
    networks:
      - llm-net

networks:
  llm-net:
    driver: bridge
EOF

echo "Generated docker-compose.yml"

# -------------------------------------------------------------------
# SAVE ADMIN PASSWORD
# -------------------------------------------------------------------
ADMIN_PW_FILE="anythingllm_admin_password.txt"
echo "${ADMIN_PASSWORD}" > "${ADMIN_PW_FILE}"
chmod 600 "${ADMIN_PW_FILE}"
echo "Saved AnythingLLM admin password to '${ADMIN_PW_FILE}' (chmod 600)."

# -------------------------------------------------------------------
# HEALTH + MODEL PULL HELPERS
# -------------------------------------------------------------------
wait_for_ollama() {
  echo
  echo "Waiting for Ollama on http://127.0.0.1:11434 ..."
  local max_attempts=$(( (MAX_WAIT_MINUTES * 60) / POLL_INTERVAL_SEC ))
  local log_every_attempts=$(( (60 + POLL_INTERVAL_SEC - 1) / POLL_INTERVAL_SEC )) # ~1 minute
  local attempt=1

  while (( attempt <= max_attempts )); do
    set +e
    HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:11434/api/tags 2>/dev/null || echo "000")
    set -e

    if [[ "${HTTP_CODE}" == "200" ]]; then
      echo "‚úÖ Ollama is online (HTTP 200 from /api/tags)."
      return 0
    fi

    if (( attempt % log_every_attempts == 0 )); then
      echo "  [${attempt}/${max_attempts}] HTTP ${HTTP_CODE} ‚Äî latest logs:"
      docker logs ollama --tail 20 2>&1 || true
    else
      echo "  [${attempt}/${max_attempts}] waiting (HTTP ${HTTP_CODE})..."
    fi

    if [[ "$(docker inspect -f '{{.State.Running}}' ollama 2>/dev/null || echo false)" != "true" ]]; then
      echo "‚ùå Ollama container is not running."
      docker logs ollama --tail 200 2>&1 || true
      return 1
    fi

    sleep "${POLL_INTERVAL_SEC}"
    attempt=$((attempt+1))
  done

  echo "‚ùå Ollama did not become ready in time (>${MAX_WAIT_MINUTES} minutes)."
  docker logs ollama --tail 200 2>&1 || true
  return 1
}

ensure_ollama_model() {
  local model="$1"
  echo
  echo "Ensuring Ollama model '${model}' is available (may take time)..."
  if docker exec ollama ollama list 2>/dev/null | grep -q "^${model}"; then
    echo "Model ${model} already present."
    return 0
  fi

  docker exec ollama ollama pull "${model}"
}

# -------------------------------------------------------------------
# START?
# -------------------------------------------------------------------
echo
echo "==============================================="
echo " AnythingLLM admin password (AUTH_TOKEN):"
echo "   ${ADMIN_PASSWORD}"
echo " Saved to: ${ADMIN_PW_FILE}"
echo " Ollama model to pull: ${OLLAMA_MODEL}"
echo " Embedding model:      ${OLLAMA_EMBED_MODEL}"
echo "==============================================="
echo

read -rp "Start Ollama + AnythingLLM + nginx now? (y/N): " START_NOW
START_NOW=${START_NOW:-n}

if [[ "${START_NOW}" =~ ^[Yy]$ ]]; then
  echo "Starting containers..."
  ${DOCKER_COMPOSE_CMD} up -d

  if ! wait_for_ollama; then
    echo
    echo "Ollama is not healthy yet; fix logs above and restart:"
    echo "  ${DOCKER_COMPOSE_CMD} up -d"
    exit 1
  fi

  # Pull chat + embedding models (long but persistent thanks to volume)
  ensure_ollama_model "${OLLAMA_MODEL}"
  ensure_ollama_model "${OLLAMA_EMBED_MODEL}"

  echo
  echo "===================================================================="
  echo " üéâ Deployment complete & Ollama is ONLINE."
  echo "--------------------------------------------------------------------"
  echo " Ollama API (local only):"
  echo "   http://127.0.0.1:11434"
  echo
  echo " AnythingLLM via HTTPS (self-signed cert):"
  echo "   https://${HOST_FQDN}/"
  echo
  echo " Use the admin password in ${ADMIN_PW_FILE} to log in."
  echo " Models are cached in ./ollama_data and survive restarts."
  echo "===================================================================="
else
  echo "Configuration complete. Start stack later with:"
  echo "  ${DOCKER_COMPOSE_CMD} up -d"
fi

echo
echo "Done. One script to rule them all (Ollama edition). üßô‚Äç‚ôÇÔ∏è"
