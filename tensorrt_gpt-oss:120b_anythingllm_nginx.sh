#!/usr/bin/env bash
set -Eeuo pipefail
trap 'echo "ERROR on line $LINENO: $BASH_COMMAND" >&2' ERR

echo "==================================================================="
echo " DGX Spark Setup: TensorRT-LLM (gpt-oss-120b) + AnythingLLM + HTTPS"
echo "==================================================================="
echo
echo "You need an NVIDIA NGC API key to pull the TensorRT-LLM container."
echo "Create/manage it here:"
echo "  üëâ  https://ngc.nvidia.com/setup/api-key"
echo

# -------------------------------------------------------------------
# REQUIREMENTS
# -------------------------------------------------------------------
for cmd in docker openssl curl; do
  if ! command -v "$cmd" >/dev/null 2>&1; then
    echo "ERROR: '$cmd' is required but not installed."
    exit 1
  fi
done

if docker compose version >/dev/null 2>&1; then
  DOCKER_COMPOSE_CMD="docker compose"
else
  if ! command -v docker-compose >/dev/null 2>&1; then
    echo "ERROR: docker compose (v2) or docker-compose (v1) is required."
    exit 1
  fi
  DOCKER_COMPOSE_CMD="docker-compose"
fi

# -------------------------------------------------------------------
# NGC API KEY + LOGIN
# -------------------------------------------------------------------
NGC_API_KEY=""
while :; do
  echo
  read -rsp "Enter your NVIDIA NGC API Key (input hidden): " NGC_API_KEY
  echo
  if [[ -z "$NGC_API_KEY" ]]; then
    echo "API key cannot be empty."
    continue
  fi

  echo "Testing NVIDIA registry login..."
  set +e
  echo "$NGC_API_KEY" | docker login nvcr.io -u '$oauthtoken' --password-stdin >/dev/null 2>&1
  LOGIN_STATUS=$?
  set -e

  if [[ $LOGIN_STATUS -eq 0 ]]; then
    echo "‚úÖ NVIDIA registry login successful."
    break
  else
    echo "‚ùå Login failed. Wrong/expired key or no registry access."
    read -rp "Try again? (y/N): " retry
    retry=${retry:-n}
    if [[ ! "$retry" =~ ^[Yy]$ ]]; then
      echo "Aborting setup."
      exit 1
    fi
  fi
done

# -------------------------------------------------------------------
# HOSTNAME + SECRETS
# -------------------------------------------------------------------
HOST_FQDN="$(hostname -f 2>/dev/null || hostname)"
echo
echo "Using hostname '${HOST_FQDN}' for TLS certificate CN and nginx server_name."
echo

echo "Generating secrets (JWT, AnythingLLM admin password)..."

JWT_SECRET="$(openssl rand -hex 64)"
ADMIN_PASSWORD="$(openssl rand -base64 32 | tr -dc 'A-Za-z0-9' | head -c 24)"

UID_VAL="$(id -u)"
GID_VAL="$(id -g)"

# Long downloads/compiles happen on first gpt-oss-120b bring-up.
: "${MAX_WAIT_MINUTES:=240}"   # 4 hours default
: "${POLL_INTERVAL_SEC:=5}"
: "${OPENAI_API_BASE:=http://trtllm-proxy:7000/v1}"
# Empirische Grenze aus TRT-Log: max attention window 81696 Tokens
: "${TRT_MAX_SEQ_LEN:=81696}"
: "${TRT_MAX_NUM_TOKENS:=81696}"
# Max prompt size 80000 tokens
: "${TRT_FREE_MEM_FRACTION:=0.95}"

# -------------------------------------------------------------------
# ENV FILES
# -------------------------------------------------------------------
cat > .env <<EOF
# Auto-generated by setup_trtllm_anythingllm.sh
NGC_API_KEY=${NGC_API_KEY}
UID=${UID_VAL}
GID=${GID_VAL}
EOF

cat > anythingllm.env <<EOF
# Auto-generated by setup_trtllm_anythingllm.sh

AUTH_TOKEN=${ADMIN_PASSWORD}
JWT_SECRET=${JWT_SECRET}
STORAGE_DIR=/app/server/storage

# Use Generic OpenAI-compatible provider, backed by local TensorRT-LLM
LLM_PROVIDER=generic-openai
GENERIC_OPEN_AI_API_KEY=tensorrt_llm
GENERIC_OPEN_AI_BASE_PATH=${OPENAI_API_BASE}
GENERIC_OPEN_AI_MODEL_PREF=gpt-oss-120b
# High ceilings so UI can down-tune per workspace
GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT=${TRT_MAX_SEQ_LEN}
GENERIC_OPEN_AI_MAX_TOKENS=2000
EOF

echo "Created .env and anythingllm.env"

# -------------------------------------------------------------------
# SSL CERT FOR NGINX
# -------------------------------------------------------------------
mkdir -p nginx/certs
echo "Generating self-signed TLS certificate..."
openssl req -x509 -nodes -days 365 \
  -newkey rsa:4096 \
  -keyout nginx/certs/anythingllm.key \
  -out nginx/certs/anythingllm.crt \
  -subj "/CN=${HOST_FQDN}" >/dev/null 2>&1

# -------------------------------------------------------------------
# NGINX CONFIG
# -------------------------------------------------------------------
mkdir -p nginx
cat > nginx/nginx.conf <<EOF
user  nginx;
worker_processes  auto;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    sendfile      on;
    keepalive_timeout  65;

    # HTTP -> HTTPS redirect
    server {
        listen 80;
        server_name ${HOST_FQDN};
        return 301 https://\$host\$request_uri;
    }

    # HTTPS reverse proxy to AnythingLLM
    server {
        listen 443 ssl;
        server_name ${HOST_FQDN};

        ssl_certificate     /etc/nginx/certs/anythingllm.crt;
        ssl_certificate_key /etc/nginx/certs/anythingllm.key;

        ssl_protocols       TLSv1.2 TLSv1.3;
        ssl_ciphers         HIGH:!aNULL:!MD5;

        location / {
            proxy_pass http://anythingllm:3001;
            proxy_http_version 1.1;

            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
            proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;

            proxy_buffering off;
        }
    }
}
EOF

# -------------------------------------------------------------------
# DIRECTORIES
# -------------------------------------------------------------------
mkdir -p trtllm_cache
mkdir -p anythingllm_data
mkdir -p trtllm_cache/tiktoken_encodings
mkdir -p proxy

# Pre-download tiktoken vocab needed by Harmony (gpt-oss tokenizer)
TIKTOKEN_DIR="trtllm_cache/tiktoken_encodings"
O200K_FILE="${TIKTOKEN_DIR}/o200k_base.tiktoken"
CL100K_FILE="${TIKTOKEN_DIR}/cl100k_base.tiktoken"

download_vocab() {
  local url="$1" dest="$2"
  if [[ ! -s "$dest" ]]; then
    echo "Downloading $(basename "$dest") ..."
    curl -L --fail --retry 3 --retry-delay 2 "$url" -o "$dest"
  else
    echo "Found $(basename "$dest") (skip download)."
  fi
}

download_vocab "https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken" "$O200K_FILE"
download_vocab "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken" "$CL100K_FILE"

# -------------------------------------------------------------------
# PROXY (context trim, future LB/queue hook)
# -------------------------------------------------------------------
cat > proxy/proxy_app.py <<'EOF'
import os
import json
import tiktoken
import requests
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

TARGET_BASE = os.getenv("TARGET_BASE", "http://trtllm-gpt-oss-120b:8000/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "openai/gpt-oss-120b")
MAX_CTX = int(os.getenv("MAX_CTX", "81696"))
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "2000"))
API_KEY = os.getenv("PROXY_API_KEY", "tensorrt_llm")

enc = tiktoken.get_encoding("cl100k_base")
app = FastAPI()


def count_tokens(text: str) -> int:
    return len(enc.encode(text))


def trim_messages(messages):
    joined = []
    for m in messages:
        role = m.get("role", "")
        content = m.get("content", "")
        if isinstance(content, list):
            content = " ".join([c.get("text", "") if isinstance(c, dict) else str(c) for c in content])
        joined.append((role, content))
    texts = [f"{r}: {c}" for r, c in joined]
    full = "\n".join(texts)
    total_tokens = count_tokens(full)
    limit = MAX_CTX - MAX_TOKENS
    if total_tokens <= limit:
        return messages, total_tokens, total_tokens, 0
    trimmed = []
    acc_tokens = 0
    for role, content in reversed(joined):
        toks = count_tokens(f"{role}: {content}")
        if acc_tokens + toks > limit:
            available = limit - acc_tokens
            if available > 0:
                ids = enc.encode(f"{role}: {content}")
                kept = enc.decode(ids[-available:])
                trimmed.append({"role": role, "content": kept})
                acc_tokens += available
            break
        else:
            trimmed.append({"role": role, "content": content})
            acc_tokens += toks
    trimmed.reverse()
    trimmed_tokens = acc_tokens
    removed_tokens = max(total_tokens - trimmed_tokens, 0)
    return trimmed, trimmed_tokens, total_tokens, removed_tokens


@app.post("/v1/chat/completions")
async def chat_completions(req: Request):
    body = await req.json()
    messages = body.get("messages", [])
    max_tokens = int(body.get("max_tokens", MAX_TOKENS))
    trimmed_messages, kept_tokens, original_tokens, removed_tokens = trim_messages(messages)
    percent_of_ctx = round(kept_tokens / max(1, (MAX_CTX - max_tokens)) * 100, 2)
    body["messages"] = trimmed_messages
    body["model"] = body.get("model", MODEL_NAME)
    body["max_tokens"] = max_tokens
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    r = requests.post(f"{TARGET_BASE}/chat/completions", headers=headers, json=body, timeout=3600)
    try:
        resp_json = r.json()
    except Exception:
        resp_json = {"error": "invalid json from backend", "backend_text": r.text}
    resp_json["proxy_token_usage"] = {
        "prompt_tokens_original": original_tokens,
        "prompt_tokens_kept": kept_tokens,
        "prompt_tokens_trimmed": removed_tokens,
        "max_context": MAX_CTX,
        "max_tokens": max_tokens,
        "prompt_window_percent_used": percent_of_ctx,
    }
    print(
        f"[proxy] original={original_tokens} kept={kept_tokens} trimmed={removed_tokens} "
        f"ctx={MAX_CTX} max_tokens={max_tokens} used={percent_of_ctx}% status={r.status_code}"
    )
    return JSONResponse(status_code=r.status_code, content=resp_json)


@app.get("/v1/models")
async def models():
    headers = {"Authorization": f"Bearer {API_KEY}"}
    r = requests.get(f"{TARGET_BASE}/models", headers=headers, timeout=30)
    return JSONResponse(status_code=r.status_code, content=r.json())


@app.get("/health")
async def health():
    return {"status": "ok", "target": TARGET_BASE}
EOF

# -------------------------------------------------------------------
# DOCKER-COMPOSE (TRT-LLM + AnythingLLM + nginx)
# -------------------------------------------------------------------
# Note: keep this heredoc unquoted so ${TRT_*} values get expanded.
cat > docker-compose.yml <<EOF
services:
  trtllm-gpt-oss-120b:
    image: nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev
    container_name: trtllm-gpt-oss-120b
    restart: unless-stopped
    # OpenAI-compatible server on 8000; bound locally on the host
    ports:
      - "127.0.0.1:8000:8000"
    shm_size: "16g"
    gpus: "all"
    environment:
      # Just in case we need access inside the container
      HF_HOME: /workspace/.cache/huggingface
      TIKTOKEN_RS_CACHE_DIR: /workspace/.cache/tiktoken_encodings
      TIKTOKEN_ENCODINGS_BASE: /workspace/.cache/tiktoken_encodings
    volumes:
      - ./trtllm_cache:/workspace/.cache
      - ./trtllm_cache/tiktoken_encodings:/workspace/.cache/tiktoken_encodings:ro
    networks:
      - llm-net
    command: >
      bash -lc "echo 'Starting trtllm-serve for openai/gpt-oss-120b...' && trtllm-serve openai/gpt-oss-120b --host 0.0.0.0 --port 8000 --backend pytorch --kv_cache_free_gpu_memory_fraction ${TRT_FREE_MEM_FRACTION} --max_batch_size 1 --max_num_tokens ${TRT_MAX_NUM_TOKENS} --max_seq_len ${TRT_MAX_SEQ_LEN}"

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    restart: unless-stopped
    cap_add:
      - SYS_ADMIN
    user: "${UID}:${GID}"
    env_file:
      - anythingllm.env
    environment:
      STORAGE_DIR: /app/server/storage
    volumes:
      - ./anythingllm_data:/app/server/storage
      - ./anythingllm.env:/app/server/.env
    networks:
      - llm-net

  nginx:
    image: nginx:alpine
    container_name: anythingllm-nginx
    restart: unless-stopped
    depends_on:
      - anythingllm
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
    networks:
      - llm-net

  trtllm-proxy:
    image: python:3.11-slim
    container_name: trtllm-proxy
    restart: unless-stopped
    depends_on:
      - trtllm-gpt-oss-120b
    ports:
      - "127.0.0.1:7000:7000"
    environment:
      TARGET_BASE: http://trtllm-gpt-oss-120b:8000/v1
      MODEL_NAME: openai/gpt-oss-120b
      MAX_CTX: ${TRT_MAX_SEQ_LEN}
      MAX_TOKENS: 2000
      PROXY_API_KEY: tensorrt_llm
    volumes:
      - ./proxy_app.py:/app/proxy_app.py:ro
    working_dir: /app
    command: bash -lc "pip install --no-cache-dir fastapi uvicorn tiktoken requests && uvicorn proxy_app:app --host 0.0.0.0 --port 7000"
    networks:
      - llm-net

networks:
  llm-net:
    driver: bridge
EOF

echo "Generated docker-compose.yml"

# -------------------------------------------------------------------
# SAVE ADMIN PASSWORD
# -------------------------------------------------------------------
ADMIN_PW_FILE="anythingllm_admin_password.txt"
echo "${ADMIN_PASSWORD}" > "${ADMIN_PW_FILE}"
chmod 600 "${ADMIN_PW_FILE}"
echo "Saved AnythingLLM admin password to '${ADMIN_PW_FILE}' (chmod 600)."

# -------------------------------------------------------------------
# TRT-LLM HEALTH CHECK
# -------------------------------------------------------------------
wait_for_trtllm() {
  echo
  echo "Waiting for TensorRT-LLM (gpt-oss-120b) on http://127.0.0.1:8000/v1 ..."
  echo "This may take a long time the first run (weights + compile). Timeout: ${MAX_WAIT_MINUTES} minutes."

  local max_attempts=$(( (MAX_WAIT_MINUTES * 60) / POLL_INTERVAL_SEC ))
  local log_every_attempts=$(( (60 + POLL_INTERVAL_SEC - 1) / POLL_INTERVAL_SEC )) # ~1 minute
  local attempt=1

  while (( attempt <= max_attempts )); do
    set +e
    HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
      -H "Authorization: Bearer tensorrt_llm" \
      http://127.0.0.1:8000/v1/models 2>/dev/null || echo "000")
    set -e

    if [[ "${HTTP_CODE}" == "200" ]]; then
      echo "‚úÖ TensorRT-LLM is online (HTTP 200 from /v1/models)."
      echo "Models:"
      curl -s -H "Authorization: Bearer tensorrt_llm" http://127.0.0.1:8000/v1/models || true
      echo
      return 0
    fi

    if (( attempt % log_every_attempts == 0 )); then
      echo "  [${attempt}/${max_attempts}] HTTP ${HTTP_CODE} ‚Äî latest logs:"
      docker logs trtllm-gpt-oss-120b --tail 5 2>&1 || true
    else
      echo "  [${attempt}/${max_attempts}] waiting (HTTP ${HTTP_CODE})..."
    fi

    if [[ "$(docker inspect -f '{{.State.Running}}' trtllm-gpt-oss-120b 2>/dev/null || echo false)" != "true" ]]; then
      echo "‚ùå TensorRT-LLM container is not running."
      docker logs trtllm-gpt-oss-120b --tail 200 2>&1 || true
      return 1
    fi

    sleep "${POLL_INTERVAL_SEC}"
    attempt=$((attempt+1))
  done

  echo "‚ùå TensorRT-LLM did not become ready in time (>${MAX_WAIT_MINUTES} minutes)."
  docker logs trtllm-gpt-oss-120b --tail 200 2>&1 || true
  return 1
}

# -------------------------------------------------------------------
# START?
# -------------------------------------------------------------------
echo
echo "==============================================="
echo " AnythingLLM admin password (AUTH_TOKEN):"
echo "   ${ADMIN_PASSWORD}"
echo " Saved to: ${ADMIN_PW_FILE}"
echo "==============================================="
echo

read -rp "Start TensorRT-LLM + AnythingLLM + nginx now? (y/N): " START_NOW
START_NOW=${START_NOW:-n}

if [[ "${START_NOW}" =~ ^[Yy]$ ]]; then
  echo "Starting containers..."
  ${DOCKER_COMPOSE_CMD} up -d

  if ! wait_for_trtllm; then
    echo
    echo "TensorRT-LLM is not healthy yet; fix its logs above and restart:"
    echo "  ${DOCKER_COMPOSE_CMD} up -d"
    exit 1
  fi

  echo
  echo "===================================================================="
  echo " üéâ Deployment complete & TensorRT-LLM is ONLINE."
  echo "--------------------------------------------------------------------"
  echo " TRT-LLM gpt-oss-120b API (local only):"
  echo "   http://127.0.0.1:8000/v1/chat/completions"
  echo
  echo " AnythingLLM via HTTPS (self-signed cert):"
  echo "   https://${HOST_FQDN}/"
  echo
  echo " Use the admin password in ${ADMIN_PW_FILE} to log in."
  echo "===================================================================="
else
  echo "Configuration complete. Start stack later with:"
  echo "  ${DOCKER_COMPOSE_CMD} up -d"
fi

echo
echo "Done. One script to rule them all (TensorRT-LLM edition). üßô‚Äç‚ôÇÔ∏è"
