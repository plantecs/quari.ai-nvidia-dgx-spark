#!/usr/bin/env bash
set -Eeuo pipefail
trap 'echo "ERROR on line $LINENO: $BASH_COMMAND" >&2' ERR

echo "==================================================================="
echo " DGX Spark Setup: TensorRT-LLM (gpt-oss-120b) + AnythingLLM + HTTPS"
echo "==================================================================="
echo
echo "You need an NVIDIA NGC API key to pull the TensorRT-LLM container."
echo "Create/manage it here:"
echo "  üëâ  https://ngc.nvidia.com/setup/api-key"
echo

# -------------------------------------------------------------------
# REQUIREMENTS
# -------------------------------------------------------------------
for cmd in docker openssl curl; do
  if ! command -v "$cmd" >/dev/null 2>&1; then
    echo "ERROR: '$cmd' is required but not installed."
    exit 1
  fi
done

if docker compose version >/dev/null 2>&1; then
  DOCKER_COMPOSE_CMD="docker compose"
else
  if ! command -v docker-compose >/dev/null 2>&1; then
    echo "ERROR: docker compose (v2) or docker-compose (v1) is required."
    exit 1
  fi
  DOCKER_COMPOSE_CMD="docker-compose"
fi

# -------------------------------------------------------------------
# NGC API KEY + LOGIN
# -------------------------------------------------------------------
NGC_API_KEY=""
while :; do
  echo
  read -rsp "Enter your NVIDIA NGC API Key (input hidden): " NGC_API_KEY
  echo
  if [[ -z "$NGC_API_KEY" ]]; then
    echo "API key cannot be empty."
    continue
  fi

  echo "Testing NVIDIA registry login..."
  set +e
  echo "$NGC_API_KEY" | docker login nvcr.io -u '$oauthtoken' --password-stdin >/dev/null 2>&1
  LOGIN_STATUS=$?
  set -e

  if [[ $LOGIN_STATUS -eq 0 ]]; then
    echo "‚úÖ NVIDIA registry login successful."
    break
  else
    echo "‚ùå Login failed. Wrong/expired key or no registry access."
    read -rp "Try again? (y/N): " retry
    retry=${retry:-n}
    if [[ ! "$retry" =~ ^[Yy]$ ]]; then
      echo "Aborting setup."
      exit 1
    fi
  fi
done

# -------------------------------------------------------------------
# HOSTNAME + SECRETS
# -------------------------------------------------------------------
HOST_FQDN="$(hostname -f 2>/dev/null || hostname)"
echo
echo "Using hostname '${HOST_FQDN}' for TLS certificate CN and nginx server_name."
echo

echo "Generating secrets (JWT, AnythingLLM admin password)..."

JWT_SECRET="$(openssl rand -hex 64)"
ADMIN_PASSWORD="$(openssl rand -base64 32 | tr -dc 'A-Za-z0-9' | head -c 24)"

UID_VAL="$(id -u)"
GID_VAL="$(id -g)"

# Long downloads/compiles happen on first gpt-oss-120b bring-up.
: "${MAX_WAIT_MINUTES:=240}"   # 4 hours default
: "${POLL_INTERVAL_SEC:=5}"
: "${OPENAI_API_BASE:=http://trtllm-gpt-oss-120b:8000/v1}"
: "${TRT_MAX_SEQ_LEN:=82000}"
: "${TRT_MAX_NUM_TOKENS:=84000}"
# Max prompt size 80000 tokens
: "${TRT_FREE_MEM_FRACTION:=0.95}"

# -------------------------------------------------------------------
# ENV FILES
# -------------------------------------------------------------------
cat > .env <<EOF
# Auto-generated by setup_trtllm_anythingllm.sh
NGC_API_KEY=${NGC_API_KEY}
UID=${UID_VAL}
GID=${GID_VAL}
EOF

cat > anythingllm.env <<EOF
# Auto-generated by setup_trtllm_anythingllm.sh

AUTH_TOKEN=${ADMIN_PASSWORD}
JWT_SECRET=${JWT_SECRET}
STORAGE_DIR=/app/server/storage

# Use Generic OpenAI-compatible provider, backed by local TensorRT-LLM
LLM_PROVIDER=generic-openai
GENERIC_OPEN_AI_API_KEY=tensorrt_llm
GENERIC_OPEN_AI_BASE_PATH=${OPENAI_API_BASE}
GENERIC_OPEN_AI_MODEL_PREF=gpt-oss-120b
# High ceilings so UI can down-tune per workspace
GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT=${TRT_MAX_SEQ_LEN}
GENERIC_OPEN_AI_MAX_TOKENS=2000
EOF

echo "Created .env and anythingllm.env"

# -------------------------------------------------------------------
# SSL CERT FOR NGINX
# -------------------------------------------------------------------
mkdir -p nginx/certs
echo "Generating self-signed TLS certificate..."
openssl req -x509 -nodes -days 365 \
  -newkey rsa:4096 \
  -keyout nginx/certs/anythingllm.key \
  -out nginx/certs/anythingllm.crt \
  -subj "/CN=${HOST_FQDN}" >/dev/null 2>&1

# -------------------------------------------------------------------
# NGINX CONFIG
# -------------------------------------------------------------------
mkdir -p nginx
cat > nginx/nginx.conf <<EOF
user  nginx;
worker_processes  auto;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    sendfile      on;
    keepalive_timeout  65;

    # HTTP -> HTTPS redirect
    server {
        listen 80;
        server_name ${HOST_FQDN};
        return 301 https://\$host\$request_uri;
    }

    # HTTPS reverse proxy to AnythingLLM
    server {
        listen 443 ssl;
        server_name ${HOST_FQDN};

        ssl_certificate     /etc/nginx/certs/anythingllm.crt;
        ssl_certificate_key /etc/nginx/certs/anythingllm.key;

        ssl_protocols       TLSv1.2 TLSv1.3;
        ssl_ciphers         HIGH:!aNULL:!MD5;

        location / {
            proxy_pass http://anythingllm:3001;
            proxy_http_version 1.1;

            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
            proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;

            proxy_buffering off;
        }
    }
}
EOF

# -------------------------------------------------------------------
# DIRECTORIES
# -------------------------------------------------------------------
mkdir -p trtllm_cache
mkdir -p anythingllm_data
mkdir -p trtllm_cache/tiktoken_encodings

# Pre-download tiktoken vocab needed by Harmony (gpt-oss tokenizer)
TIKTOKEN_DIR="trtllm_cache/tiktoken_encodings"
O200K_FILE="${TIKTOKEN_DIR}/o200k_base.tiktoken"
CL100K_FILE="${TIKTOKEN_DIR}/cl100k_base.tiktoken"

download_vocab() {
  local url="$1" dest="$2"
  if [[ ! -s "$dest" ]]; then
    echo "Downloading $(basename "$dest") ..."
    curl -L --fail --retry 3 --retry-delay 2 "$url" -o "$dest"
  else
    echo "Found $(basename "$dest") (skip download)."
  fi
}

download_vocab "https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken" "$O200K_FILE"
download_vocab "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken" "$CL100K_FILE"

# -------------------------------------------------------------------
# DOCKER-COMPOSE (TRT-LLM + AnythingLLM + nginx)
# -------------------------------------------------------------------
cat > docker-compose.yml <<'EOF'
services:
  trtllm-gpt-oss-120b:
    image: nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev
    container_name: trtllm-gpt-oss-120b
    restart: unless-stopped
    # OpenAI-compatible server on 8000; bound locally on the host
    ports:
      - "127.0.0.1:8000:8000"
    shm_size: "16g"
    gpus: "all"
    environment:
      # Just in case we need access inside the container
      HF_HOME: /workspace/.cache/huggingface
      TIKTOKEN_RS_CACHE_DIR: /workspace/.cache/tiktoken_encodings
      TIKTOKEN_ENCODINGS_BASE: /workspace/.cache/tiktoken_encodings
    volumes:
      - ./trtllm_cache:/workspace/.cache
      - ./trtllm_cache/tiktoken_encodings:/workspace/.cache/tiktoken_encodings:ro
    networks:
      - llm-net
    command: >
      bash -lc "echo 'Starting trtllm-serve for openai/gpt-oss-120b...' && trtllm-serve openai/gpt-oss-120b --host 0.0.0.0 --port 8000 --backend pytorch --max_batch_size 1 --max_num_tokens ${TRT_MAX_NUM_TOKENS} --max_seq_len ${TRT_MAX_SEQ_LEN}"

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    restart: unless-stopped
    cap_add:
      - SYS_ADMIN
    user: "${UID}:${GID}"
    env_file:
      - anythingllm.env
    environment:
      STORAGE_DIR: /app/server/storage
    volumes:
      - ./anythingllm_data:/app/server/storage
      - ./anythingllm.env:/app/server/.env
    networks:
      - llm-net

  nginx:
    image: nginx:alpine
    container_name: anythingllm-nginx
    restart: unless-stopped
    depends_on:
      - anythingllm
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
    networks:
      - llm-net

networks:
  llm-net:
    driver: bridge
EOF

echo "Generated docker-compose.yml"

# -------------------------------------------------------------------
# SAVE ADMIN PASSWORD
# -------------------------------------------------------------------
ADMIN_PW_FILE="anythingllm_admin_password.txt"
echo "${ADMIN_PASSWORD}" > "${ADMIN_PW_FILE}"
chmod 600 "${ADMIN_PW_FILE}"
echo "Saved AnythingLLM admin password to '${ADMIN_PW_FILE}' (chmod 600)."

# -------------------------------------------------------------------
# TRT-LLM HEALTH CHECK
# -------------------------------------------------------------------
wait_for_trtllm() {
  echo
  echo "Waiting for TensorRT-LLM (gpt-oss-120b) on http://127.0.0.1:8000/v1 ..."
  echo "This may take a long time the first run (weights + compile). Timeout: ${MAX_WAIT_MINUTES} minutes."

  local max_attempts=$(( (MAX_WAIT_MINUTES * 60) / POLL_INTERVAL_SEC ))
  local log_every_attempts=$(( (60 + POLL_INTERVAL_SEC - 1) / POLL_INTERVAL_SEC )) # ~1 minute
  local attempt=1

  while (( attempt <= max_attempts )); do
    set +e
    HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
      -H "Authorization: Bearer tensorrt_llm" \
      http://127.0.0.1:8000/v1/models 2>/dev/null || echo "000")
    set -e

    if [[ "${HTTP_CODE}" == "200" ]]; then
      echo "‚úÖ TensorRT-LLM is online (HTTP 200 from /v1/models)."
      echo "Models:"
      curl -s -H "Authorization: Bearer tensorrt_llm" http://127.0.0.1:8000/v1/models || true
      echo
      return 0
    fi

    if (( attempt % log_every_attempts == 0 )); then
      echo "  [${attempt}/${max_attempts}] HTTP ${HTTP_CODE} ‚Äî latest logs:"
      docker logs trtllm-gpt-oss-120b --tail 5 2>&1 || true
    else
      echo "  [${attempt}/${max_attempts}] waiting (HTTP ${HTTP_CODE})..."
    fi

    if [[ "$(docker inspect -f '{{.State.Running}}' trtllm-gpt-oss-120b 2>/dev/null || echo false)" != "true" ]]; then
      echo "‚ùå TensorRT-LLM container is not running."
      docker logs trtllm-gpt-oss-120b --tail 200 2>&1 || true
      return 1
    fi

    sleep "${POLL_INTERVAL_SEC}"
    attempt=$((attempt+1))
  done

  echo "‚ùå TensorRT-LLM did not become ready in time (>${MAX_WAIT_MINUTES} minutes)."
  docker logs trtllm-gpt-oss-120b --tail 200 2>&1 || true
  return 1
}

# -------------------------------------------------------------------
# START?
# -------------------------------------------------------------------
echo
echo "==============================================="
echo " AnythingLLM admin password (AUTH_TOKEN):"
echo "   ${ADMIN_PASSWORD}"
echo " Saved to: ${ADMIN_PW_FILE}"
echo "==============================================="
echo

read -rp "Start TensorRT-LLM + AnythingLLM + nginx now? (y/N): " START_NOW
START_NOW=${START_NOW:-n}

if [[ "${START_NOW}" =~ ^[Yy]$ ]]; then
  echo "Starting containers..."
  ${DOCKER_COMPOSE_CMD} up -d

  if ! wait_for_trtllm; then
    echo
    echo "TensorRT-LLM is not healthy yet; fix its logs above and restart:"
    echo "  ${DOCKER_COMPOSE_CMD} up -d"
    exit 1
  fi

  echo
  echo "===================================================================="
  echo " üéâ Deployment complete & TensorRT-LLM is ONLINE."
  echo "--------------------------------------------------------------------"
  echo " TRT-LLM gpt-oss-120b API (local only):"
  echo "   http://127.0.0.1:8000/v1/chat/completions"
  echo
  echo " AnythingLLM via HTTPS (self-signed cert):"
  echo "   https://${HOST_FQDN}/"
  echo
  echo " Use the admin password in ${ADMIN_PW_FILE} to log in."
  echo "===================================================================="
else
  echo "Configuration complete. Start stack later with:"
  echo "  ${DOCKER_COMPOSE_CMD} up -d"
fi

echo
echo "Done. One script to rule them all (TensorRT-LLM edition). üßô‚Äç‚ôÇÔ∏è"
